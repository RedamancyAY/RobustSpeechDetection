{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caf012ad-3972-4f7c-912b-03394bda3388",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T03:42:08.326225Z",
     "iopub.status.busy": "2024-09-22T03:42:08.325983Z",
     "iopub.status.idle": "2024-09-22T03:42:08.348651Z",
     "shell.execute_reply": "2024-09-22T03:42:08.347794Z",
     "shell.execute_reply.started": "2024-09-22T03:42:08.326205Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "882cdde4-14f0-4ed7-8a56-ded709a8784c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T04:00:34.287045Z",
     "iopub.status.busy": "2024-09-22T04:00:34.286476Z",
     "iopub.status.idle": "2024-09-22T04:00:34.349018Z",
     "shell.execute_reply": "2024-09-22T04:00:34.347209Z",
     "shell.execute_reply.started": "2024-09-22T04:00:34.286983Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoFeatureExtractor, Wav2Vec2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b459dc3-b5ef-4a27-a3ba-16063a60d954",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T04:05:09.214111Z",
     "iopub.status.busy": "2024-09-22T04:05:09.213417Z",
     "iopub.status.idle": "2024-09-22T04:05:09.291118Z",
     "shell.execute_reply": "2024-09-22T04:05:09.289262Z",
     "shell.execute_reply.started": "2024-09-22T04:05:09.214047Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerBaseLine(nn.Module):\n",
    "    def __init__(self, pretrain_feat=\"extract_features\"):\n",
    "        super().__init__()\n",
    "\n",
    "        assert pretrain_feat in [\"last_hidden_state\", \"extract_features\"]\n",
    "        self.pretrain_feat = pretrain_feat\n",
    "        # The channels of used features for the pretrained model is 512 when using\n",
    "        # the 'extract_features',  but 768 when [\"last_hidden_state\"] is used.\n",
    "        C_features = 512 if pretrain_feat == \"extract_features\" else 768\n",
    "\n",
    "        self.pretrain_model = Wav2Vec2Model.from_pretrained(\n",
    "            \"/usr/local/ay_data/0-model_weights/models--facebook--wav2vec2-base-960h\"\n",
    "        )\n",
    "\n",
    "    def build_final_block(self):\n",
    "        copied_layers = [deepcopy(self.pretrain_model.encoder.layers[i]) for i in range(6, 12)]\n",
    "        self.copied_transformer = nn.ModuleList(copied_layers)\n",
    "\n",
    "    def copy_final_stage(self):\n",
    "        # self.block4_copied = self.build_final_block()\n",
    "        self.build_final_block()\n",
    "\n",
    "    def extract_feature(self, x):\n",
    "        extract_features = self.pretrain_model.feature_extractor(x)\n",
    "        extract_features = extract_features.transpose(1, 2)\n",
    "\n",
    "        hidden_states, extract_features = self.pretrain_model.feature_projection(extract_features)\n",
    "        hidden_states = self.pretrain_model._mask_hidden_states(\n",
    "            hidden_states, mask_time_indices=None, attention_mask=None\n",
    "        )\n",
    "\n",
    "        #### split encoder process\n",
    "        encoder = self.pretrain_model.encoder\n",
    "\n",
    "        position_embeddings = encoder.pos_conv_embed(hidden_states)\n",
    "        hidden_states = hidden_states + position_embeddings\n",
    "        hidden_states = encoder.layer_norm(hidden_states)\n",
    "        hidden_states = encoder.dropout(hidden_states)\n",
    "        #### In original Wav2Vec, encoder has 12 layers\n",
    "        for layer in encoder.layers:\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            dropout_probability = torch.rand([])\n",
    "\n",
    "            skip_the_layer = True if self.training and (dropout_probability < encoder.config.layerdrop) else False\n",
    "            if not skip_the_layer:\n",
    "                layer_outputs = layer(hidden_states, attention_mask=None, output_attentions=None)\n",
    "                hidden_states = layer_outputs[0]\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "    def get_main_stem(self):\n",
    "        encoder = self.pretrain_model.encoder\n",
    "        return [\n",
    "            self.pretrain_model.feature_extractor,\n",
    "            self.pretrain_model.feature_projection,\n",
    "            encoder.pos_conv_embed, encoder.layer_norm, encoder.layers[0:6]\n",
    "        ]\n",
    "\n",
    "    def get_content_stem(self):\n",
    "        encoder = self.pretrain_model.encoder\n",
    "        return [encoder.layers[6:]]\n",
    "\n",
    "    def get_vocoder_stem(self):\n",
    "        return [self.copied_transformer]\n",
    "\n",
    "    def preprocess(self, x, **kwargs):\n",
    "        return x[:, 0, :]\n",
    "    \n",
    "    def get_hidden_state(self, x):\n",
    "        extract_features = self.pretrain_model.feature_extractor(x)\n",
    "        extract_features = extract_features.transpose(1, 2)\n",
    "\n",
    "        hidden_states, extract_features = self.pretrain_model.feature_projection(extract_features)\n",
    "        hidden_states = self.pretrain_model._mask_hidden_states(\n",
    "            hidden_states, mask_time_indices=None, attention_mask=None\n",
    "        )\n",
    "\n",
    "        #### split encoder process\n",
    "        encoder = self.pretrain_model.encoder\n",
    "\n",
    "        position_embeddings = encoder.pos_conv_embed(hidden_states)\n",
    "        hidden_states = hidden_states + position_embeddings\n",
    "        hidden_states = encoder.layer_norm(hidden_states)\n",
    "        hidden_states = encoder.dropout(hidden_states)\n",
    "        #### In original Wav2Vec, encoder has 12 layers\n",
    "        for layer in encoder.layers[0:6]:\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            dropout_probability = torch.rand([])\n",
    "\n",
    "            skip_the_layer = True if self.training and (dropout_probability < encoder.config.layerdrop) else False\n",
    "            if not skip_the_layer:\n",
    "                layer_outputs = layer(hidden_states, attention_mask=None, output_attentions=None)\n",
    "                hidden_states = layer_outputs[0]\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "    def get_final_feature(self, hidden_states):\n",
    "        encoder = self.pretrain_model.encoder\n",
    "        for layer in encoder.layers[6:]:\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            dropout_probability = torch.rand([])\n",
    "\n",
    "            skip_the_layer = True if self.training and (dropout_probability < encoder.config.layerdrop) else False\n",
    "            if not skip_the_layer:\n",
    "                layer_outputs = layer(hidden_states, attention_mask=None, output_attentions=None)\n",
    "                hidden_states = layer_outputs[0]\n",
    "\n",
    "        return hidden_states.mean(1)\n",
    "\n",
    "    def get_final_feature_copyed(self, hidden_states):\n",
    "        encoder = self.pretrain_model.encoder\n",
    "        for layer in self.copied_transformer:\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            dropout_probability = torch.rand([])\n",
    "\n",
    "            skip_the_layer = True if self.training and (dropout_probability < encoder.config.layerdrop) else False\n",
    "            if not skip_the_layer:\n",
    "                layer_outputs = layer(hidden_states, attention_mask=None, output_attentions=None)\n",
    "                hidden_states = layer_outputs[0]\n",
    "\n",
    "        return hidden_states.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4552a42-6ed4-40dc-8d6d-3d9d1197df44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T04:05:10.206435Z",
     "iopub.status.busy": "2024-09-22T04:05:10.205916Z",
     "iopub.status.idle": "2024-09-22T04:05:11.092689Z",
     "shell.execute_reply": "2024-09-22T04:05:11.091824Z",
     "shell.execute_reply.started": "2024-09-22T04:05:10.206384Z"
    },
    "scrolled": true,
    "tags": [
     "active-ipynb",
     "style-student"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at /usr/local/ay_data/0-model_weights/models--facebook--wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-9.8749e-02, -1.7376e-02,  2.5778e-01,  ..., -1.0865e-01,\n",
       "          -6.8892e-03, -1.7159e-01],\n",
       "         [-9.5807e-02, -1.5792e-02,  2.5806e-01,  ..., -1.0220e-01,\n",
       "          -9.5984e-03, -1.6934e-01],\n",
       "         [-9.4965e-02, -1.8909e-02,  2.5308e-01,  ..., -9.7469e-02,\n",
       "          -3.2506e-03, -1.7182e-01],\n",
       "         ...,\n",
       "         [-9.7951e-02, -1.5857e-02,  2.5040e-01,  ..., -1.0613e-01,\n",
       "          -3.4473e-03, -1.7160e-01],\n",
       "         [-9.7289e-02, -1.5004e-02,  2.5057e-01,  ..., -1.0719e-01,\n",
       "          -5.3351e-03, -1.7056e-01],\n",
       "         [-9.5800e-02, -1.5553e-02,  2.5213e-01,  ..., -1.0299e-01,\n",
       "          -5.1184e-03, -1.7331e-01]],\n",
       "\n",
       "        [[-9.3925e-02, -1.3120e-02,  2.2336e-01,  ..., -9.2711e-02,\n",
       "           6.2209e-03, -1.4708e-01],\n",
       "         [-9.2376e-02, -1.3820e-02,  2.1882e-01,  ..., -8.3005e-02,\n",
       "           9.1521e-03, -1.4644e-01],\n",
       "         [-9.7433e-02, -1.3845e-02,  2.2223e-01,  ..., -9.0570e-02,\n",
       "           7.3257e-03, -1.4658e-01],\n",
       "         ...,\n",
       "         [-9.0032e-02, -1.0003e-02,  2.2270e-01,  ..., -9.3379e-02,\n",
       "           4.7492e-03, -1.4638e-01],\n",
       "         [-8.7587e-02, -1.0702e-02,  2.2045e-01,  ..., -8.6841e-02,\n",
       "           6.0146e-03, -1.4594e-01],\n",
       "         [-9.2081e-02, -1.1337e-02,  2.1936e-01,  ..., -9.4822e-02,\n",
       "           6.0970e-03, -1.4663e-01]],\n",
       "\n",
       "        [[-9.2353e-02,  3.0796e-03,  1.0484e-01,  ..., -1.1661e-01,\n",
       "          -3.7507e-02, -1.1304e-01],\n",
       "         [-9.2457e-02,  3.2871e-03,  1.0086e-01,  ..., -1.1398e-01,\n",
       "          -3.4431e-02, -1.1382e-01],\n",
       "         [-9.5191e-02,  3.3263e-03,  1.0226e-01,  ..., -1.1607e-01,\n",
       "          -3.3919e-02, -1.1269e-01],\n",
       "         ...,\n",
       "         [-9.4562e-02,  5.7652e-03,  1.0444e-01,  ..., -1.2166e-01,\n",
       "          -3.6898e-02, -1.1198e-01],\n",
       "         [-9.2328e-02,  4.6144e-03,  9.9802e-02,  ..., -1.1493e-01,\n",
       "          -3.2230e-02, -1.1428e-01],\n",
       "         [-9.4342e-02,  5.3904e-03,  9.9317e-02,  ..., -1.1625e-01,\n",
       "          -3.1233e-02, -1.1381e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-8.5202e-02, -6.9546e-03,  1.7427e-01,  ..., -1.0928e-01,\n",
       "          -1.3896e-02, -1.4195e-01],\n",
       "         [-8.5732e-02, -6.3084e-03,  1.7493e-01,  ..., -1.1052e-01,\n",
       "          -1.3574e-02, -1.4278e-01],\n",
       "         [-8.7610e-02, -6.0310e-03,  1.7609e-01,  ..., -1.1345e-01,\n",
       "          -1.3420e-02, -1.4189e-01],\n",
       "         ...,\n",
       "         [-9.2747e-02,  1.3317e-03,  1.8994e-01,  ..., -1.3348e-01,\n",
       "          -2.7506e-02, -1.3934e-01],\n",
       "         [-8.9234e-02, -1.9629e-03,  1.7884e-01,  ..., -1.1958e-01,\n",
       "          -1.8451e-02, -1.4084e-01],\n",
       "         [-8.4990e-02, -1.7297e-03,  1.7735e-01,  ..., -1.1120e-01,\n",
       "          -1.7944e-02, -1.3977e-01]],\n",
       "\n",
       "        [[-1.1007e-01, -1.2904e-02,  2.1622e-01,  ..., -1.2075e-01,\n",
       "          -1.4943e-02, -2.0407e-01],\n",
       "         [-1.0854e-01, -1.2082e-02,  2.1623e-01,  ..., -1.1549e-01,\n",
       "          -1.4684e-02, -2.0393e-01],\n",
       "         [-1.0838e-01, -1.2244e-02,  2.1526e-01,  ..., -1.1510e-01,\n",
       "          -1.3891e-02, -2.0533e-01],\n",
       "         ...,\n",
       "         [-1.1694e-01, -6.6245e-03,  2.2094e-01,  ..., -1.4094e-01,\n",
       "          -2.3165e-02, -1.9943e-01],\n",
       "         [-1.1908e-01, -6.6852e-03,  2.1822e-01,  ..., -1.4183e-01,\n",
       "          -2.1378e-02, -1.9946e-01],\n",
       "         [-1.1474e-01, -9.1257e-03,  2.1520e-01,  ..., -1.3321e-01,\n",
       "          -1.7407e-02, -2.0179e-01]],\n",
       "\n",
       "        [[-9.7093e-02, -1.4610e-02,  2.4504e-01,  ..., -1.0994e-01,\n",
       "          -2.8259e-03, -1.5845e-01],\n",
       "         [-9.3835e-02, -1.6312e-02,  2.4310e-01,  ..., -1.0666e-01,\n",
       "          -1.2242e-05, -1.5927e-01],\n",
       "         [-9.5871e-02, -1.6359e-02,  2.4074e-01,  ..., -1.0709e-01,\n",
       "           1.5761e-03, -1.5905e-01],\n",
       "         ...,\n",
       "         [-9.6950e-02, -1.2504e-02,  2.4883e-01,  ..., -1.2161e-01,\n",
       "          -9.1800e-03, -1.5780e-01],\n",
       "         [-9.3198e-02, -1.2956e-02,  2.4119e-01,  ..., -1.1047e-01,\n",
       "          -3.6453e-03, -1.5754e-01],\n",
       "         [-9.3361e-02, -1.2318e-02,  2.4160e-01,  ..., -1.0745e-01,\n",
       "          -3.0646e-03, -1.5810e-01]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(10, 69000)\n",
    "model = BaseLine()\n",
    "\n",
    "model.extract_feature(x)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
