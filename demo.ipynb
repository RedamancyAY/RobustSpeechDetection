{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from argparse import Namespace\n",
    "from copy import  deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../imgs/framework.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the image, our method processes the input data using the following steps:\n",
    "1. convert the input speech into Log-Frequency Spectorgram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/GEIL2T/Softwares/anaconda3/envs/RobustSpeechDetection/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model import AudioModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The settings of the `AudioModel` are defined in the `cfg`. Each setting is a key-value pair, where the key is the name of the setting and the value is the value of the setting. The meaning of each setting is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Network Structure Parameters**:\n",
    "   - `one_stem=False`: use only 1 stem to extract audio feature and make predictions.\n",
    "   - `feature_extractor=\"ResNet\"`: Uses ResNet as the feature extractor. One can also use `transformer` to use transformer-based features.\n",
    "   - `pretrain_transformer_path`: Path or name of the pre-trained transformer model\n",
    "   - `vocoder_classes=8`: Sets the number of vocoder classes to 8\n",
    "\n",
    "2. **Loss Function Parameters**:\n",
    "   - `use_f0_loss=False`: use F0 loss as the pesudo loss rather than using speed loss and compression loss in the content stream. Set to `False` if you want to use speed loss and compression loss in the content stream.\n",
    "   - `use_speed_loss=True`: Enables speed-related loss function\n",
    "   - `use_compression_loss=True`: Enables compression loss\n",
    "   - `use_adversarial_loss=True`: Enable adversarial loss to train the content stream\n",
    "   - `feat_con_loss=True`: Enables feature contractive loss\n",
    "\n",
    "3. **Data Augmentation and Training Strategy**:\n",
    "   - `style_shuffle=True`: Enables style shuffling\n",
    "   - `feat_shuffle=True`: Enables feature shuffling\n",
    "   - `aug_policy=\"ss\"`: Sets the data augmentation policy to \"ss\"\n",
    "   - `betas=[1, 1, 0.5, 0.5]`: Likely represents optimizer beta parameters or loss weight coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Namespace(\n",
    "    one_stem=False,\n",
    "    use_f0_loss=False,\n",
    "    use_speed_loss=True,\n",
    "    use_compression_loss=True,\n",
    "    use_adversarial_loss=True,\n",
    "    style_shuffle=True,\n",
    "    feat_shuffle=True,\n",
    "    feature_extractor=\"ResNet\",\n",
    "    pretrain_transformer_path=\"facebook/wav2vec2-base-960h\",\n",
    "    vocoder_classes=8,\n",
    "    betas=[1, 0.5, 0.5, 0.5],\n",
    "    aug_policy=\"ss\",\n",
    "    feat_con_loss=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use ResNet as featrue extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/GEIL2T/Softwares/anaconda3/envs/RobustSpeechDetection/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "demo_model = AudioModel(feature_extractor='ResNet', cfg=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectrogram shape: torch.Size([3, 1, 257, 257])\n",
      "hidden_states torch.Size([3, 256, 17, 17])\n",
      "content_feature torch.Size([3, 512])\n",
      "speed_logit torch.Size([3, 16])\n",
      "compression_logit torch.Size([3, 10])\n",
      "vocoder_feature torch.Size([3, 512])\n",
      "vocoder_logit torch.Size([3, 9])\n",
      "content_voc_logit torch.Size([3, 9])\n",
      "feature torch.Size([3, 1024])\n",
      "logit torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 1, 48000)\n",
    "spectrogram = demo_model.feature_model.preprocess(x)\n",
    "print(\"Spectrogram shape:\", spectrogram.shape)\n",
    "test_res = demo_model.forward(spectrogram, stage='test')\n",
    "for k, v in test_res.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Tranformer as the feature extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, we use the \"facebook/wav2vec2-base-960h\" as the feature extractor when `feature_extractor = \"transformer\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "cfg2 = deepcopy(cfg)  # copy cfg to demo_model2\n",
    "cfg2.feature_extractor = \"transformer\"\n",
    "demo_model2 = AudioModel(feature_extractor=cfg2.feature_extractor, cfg=cfg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio shape: torch.Size([3, 48000])\n",
      "hidden_states torch.Size([3, 149, 768])\n",
      "content_feature torch.Size([3, 768])\n",
      "speed_logit torch.Size([3, 16])\n",
      "compression_logit torch.Size([3, 10])\n",
      "vocoder_feature torch.Size([3, 768])\n",
      "vocoder_logit torch.Size([3, 9])\n",
      "content_voc_logit torch.Size([3, 9])\n",
      "feature torch.Size([3, 1536])\n",
      "logit torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 1, 48000)\n",
    "spectrogram = demo_model2.feature_model.preprocess(x)\n",
    "print(\"Audio shape:\", spectrogram.shape)\n",
    "test_res = demo_model2.forward(spectrogram, stage='test')\n",
    "for k, v in test_res.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when using model for training, you must pass the batch (a dict) to the model. Then, the model will produce the shuffle_label based on the ground truth label for the feature shuffle loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print train res\n",
      "##########\n",
      "hidden_states torch.Size([3, 256, 17, 17])\n",
      "content_feature torch.Size([3, 512])\n",
      "speed_logit torch.Size([3, 16])\n",
      "compression_logit torch.Size([3, 10])\n",
      "vocoder_feature torch.Size([3, 512])\n",
      "vocoder_logit torch.Size([3, 9])\n",
      "content_voc_logit torch.Size([3, 9])\n",
      "feature torch.Size([3, 1024])\n",
      "logit torch.Size([3])\n",
      "shuffle_logit torch.Size([3])\n",
      "########## \n",
      " print batch res\n",
      "##########\n",
      "label torch.Size([3])\n",
      "shuffle_label torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 1, 48000)\n",
    "batch = {\n",
    "    'label' : torch.randint(0,2, (3,))\n",
    "}\n",
    "spectrogram = demo_model.feature_model.preprocess(x)\n",
    "train_res = demo_model.forward(spectrogram, stage=\"train\", batch=batch)\n",
    "print(\"print train res\")\n",
    "print(\"#\"*10)\n",
    "for k, v in train_res.items():\n",
    "    print(k, v.shape)\n",
    "print(\"#\"*10, '\\n', \"print batch res\")\n",
    "print(\"#\"*10)\n",
    "for k, v in batch.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lit Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the pytorch_lightning to process the data flow, compute the loss and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import AudioModel_lit\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "from pytorch_lightning.loggers import  CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = AudioModel_lit(cfg=cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test forwarding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frist, we randomly generate a batch of audio samples. Warning, the batch must have speed_label and compression_label if you use the speed_loss and compression_loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 1, 48000)\n",
    "batch = {\n",
    "    \"label\": torch.randint(0, 2, (3,)),\n",
    "    \"audio\": x,\n",
    "    \"sample_rate\": 16000,\n",
    "    \"speed_label\": torch.randint(0, 10, (3,)),\n",
    "    \"compression_label\": torch.randint(0, 10, (3,)),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our speed transformation, if the audio is not speed-changed, its speed label will be 5, while the compression label will be 0 if the audio is not compressed. When preparing your dataloader, you can use the following code to generate the speed label and compression label for each audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import RandomAudioCompressionSpeedChanging\n",
    "\n",
    "speed_compression_transform = RandomAudioCompressionSpeedChanging(p_compression=0.9, sample_rate=16000, p_speed=1.0, min_speed=0.5, max_speed=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we read metadata from the audio datasets, and get the following data and metadata for a audio sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 48000)\n",
    "metadata = {\n",
    "    \"label\": 1,\n",
    "    \"audio\": x,\n",
    "    \"sample_rate\": 16000,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the folowing code to randomly change the speed and compress audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 1\n",
      "audio tensor([[-0.6149, -0.0170,  1.6779,  ..., -0.2504,  0.3575,  1.1373]])\n",
      "sample_rate 16000\n",
      "compression_label 0\n",
      "speed_label 10\n",
      "speed 1.5\n"
     ]
    }
   ],
   "source": [
    "x = speed_compression_transform(x, metadata)\n",
    "for k, v in metadata.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, you have to pass the metadata into `speed_compression_transform` function to get the speed and compression labels.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lit model, we use the `_shared_pred` method to predict the logits of the input batch. If the stage is train, we also the the audio_transform to augment the spectrogram.\n",
    "\n",
    "\n",
    "```python\n",
    "    def _shared_pred(self, batch, batch_idx, stage=\"train\"):\n",
    "        \"\"\"common predict step for train/val/test\n",
    "\n",
    "        Note that the data augmenation is done in the self.model.feature_extractor.\n",
    "\n",
    "        \"\"\"\n",
    "        audio, sample_rate = batch[\"audio\"], batch[\"sample_rate\"]\n",
    "\n",
    "\n",
    "        audio = self.model.feature_model.preprocess(audio, stage=stage)\n",
    "        if stage == \"train\" and self.cfg.feature_extractor == \"ResNet\":\n",
    "            audio = self.audio_transform.batch_apply(audio)\n",
    "\n",
    "\n",
    "        batch_res = self.model(\n",
    "            audio,\n",
    "            stage=stage,\n",
    "            batch=batch if stage == \"train\" else None,\n",
    "            one_stem=self.one_stem,\n",
    "        )\n",
    "\n",
    "        batch_res[\"pred\"] = (torch.sigmoid(batch_res[\"logit\"]) + 0.5).int()\n",
    "\n",
    "        return batch_res\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_states torch.Size([3, 256, 17, 17])\n",
      "content_feature torch.Size([3, 512])\n",
      "speed_logit torch.Size([3, 16])\n",
      "compression_logit torch.Size([3, 10])\n",
      "vocoder_feature torch.Size([3, 512])\n",
      "vocoder_logit torch.Size([3, 9])\n",
      "content_voc_logit torch.Size([3, 9])\n",
      "feature torch.Size([3, 1024])\n",
      "logit torch.Size([3])\n",
      "shuffle_logit torch.Size([3])\n",
      "pred torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "batch_res = lit_model._shared_pred(batch=batch, batch_idx=0)\n",
    "for key, value in batch_res.items():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first build a simple dataloaders for training, where all the samples are randomly generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import EER_Callback, BinaryAUC_Callback, BinaryACC_Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleTestDataset(Dataset):\n",
    "    def __init__(self, num_samples=10):\n",
    "        # Generate synthetic data similar to your example\n",
    "        self.samples = []\n",
    "        for _ in range(num_samples):\n",
    "            self.samples.append({\n",
    "                \"audio\": torch.randn(1, 48000),\n",
    "                \"label\": torch.randint(0, 2, (1,)).item(),\n",
    "                \"sample_rate\": 16000,\n",
    "                \"speed_label\": torch.randint(0, 10, (1,)).item(),\n",
    "                \"compression_label\": torch.randint(0, 10, (1,)).item(),\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "# Create a simple collate function\n",
    "def simple_collate_fn(batch):\n",
    "    audio = torch.stack([item[\"audio\"] for item in batch])\n",
    "    label = torch.tensor([item[\"label\"] for item in batch])\n",
    "    speed_label = torch.tensor([item[\"speed_label\"] for item in batch])\n",
    "    compression_label = torch.tensor([item[\"compression_label\"] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        \"audio\": audio,\n",
    "        \"label\": label,\n",
    "        \"sample_rate\": 16000,\n",
    "        \"speed_label\": speed_label,\n",
    "        \"compression_label\": compression_label,\n",
    "    }\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "test_dataset = SimpleTestDataset(num_samples=20)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=3,\n",
    "    shuffle=False,\n",
    "    collate_fn=simple_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a simple trainer to train and test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    logger=CSVLogger(save_dir=\"./logs\", version=0),\n",
    "    max_epochs=4,\n",
    "    callbacks=[\n",
    "        BinaryACC_Callback(batch_key=\"label\", output_key=\"logit\"),\n",
    "        BinaryAUC_Callback(batch_key=\"label\", output_key=\"logit\"),\n",
    "        EER_Callback(batch_key=\"label\", output_key=\"logit\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/GEIL2T/Softwares/anaconda3/envs/RobustSpeechDetection/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "/Volumes/GEIL2T/Softwares/anaconda3/envs/RobustSpeechDetection/lib/python3.9/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory ./logs/lightning_logs/version_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "/Volumes/GEIL2T/Softwares/anaconda3/envs/RobustSpeechDetection/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory ./logs/lightning_logs/version_0/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name           | Type                    | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | model          | AudioModel              | 21.7 M | train\n",
      "1 | bce_loss       | BCEWithLogitsLoss       | 0      | train\n",
      "2 | contrast_loss2 | BinaryTokenContrastLoss | 0      | train\n",
      "3 | contrast_lossN | MultiClass_ContrastLoss | 0      | train\n",
      "4 | ce_loss        | CrossEntropyLoss        | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "21.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.7 M    Total params\n",
      "86.739    Total estimated model params size (MB)\n",
      "106       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/Volumes/GEIL2T/Softwares/anaconda3/envs/RobustSpeechDetection/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/Volumes/GEIL2T/Softwares/anaconda3/envs/RobustSpeechDetection/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 7/7 [00:01<00:00,  3.79it/s, v_num=0, train-cls_loss=0.743, train-feat_shuffle_loss=0.0672, train-vocoder_stem_loss=0.000, train-compression_loss=2.190, train-speed_loss=2.540, train-f0_loss=0.000, train-feat_contrast_loss=0.310, train-loss=3.270, train-acc=0.550, train-auc=0.770, train-eer=0.200]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 7/7 [00:02<00:00,  2.78it/s, v_num=0, train-cls_loss=0.743, train-feat_shuffle_loss=0.0672, train-vocoder_stem_loss=0.000, train-compression_loss=2.190, train-speed_loss=2.540, train-f0_loss=0.000, train-feat_contrast_loss=0.310, train-loss=3.270, train-acc=0.550, train-auc=0.770, train-eer=0.200]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(lit_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, you can view the logging loss in the logger file, for example `logs/lightning_logs/version_0/metrics.csv`.\n",
    "![](imgs/loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing, the results will also saved in logger file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/GEIL2T/Softwares/anaconda3/envs/RobustSpeechDetection/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 7/7 [00:00<00:00, 33.88it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test-acc                    0.5\n",
      "        test-auc            0.9800000190734863\n",
      "      test-cls_loss         0.6732991933822632\n",
      "  test-compression_loss     2.1219122409820557\n",
      "        test-eer            0.10000000149011612\n",
      "      test-f0_loss                  0.0\n",
      " test-feat_contrast_loss    0.32317954301834106\n",
      "        test-loss           3.1648013591766357\n",
      "     test-speed_loss        2.5379128456115723\n",
      " test-vocoder_stem_loss             0.0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test-cls_loss': 0.6732991933822632,\n",
       "  'test-vocoder_stem_loss': 0.0,\n",
       "  'test-compression_loss': 2.1219122409820557,\n",
       "  'test-speed_loss': 2.5379128456115723,\n",
       "  'test-f0_loss': 0.0,\n",
       "  'test-feat_contrast_loss': 0.32317954301834106,\n",
       "  'test-loss': 3.1648013591766357,\n",
       "  'test-acc': 0.5,\n",
       "  'test-auc': 0.9800000190734863,\n",
       "  'test-eer': 0.10000000149011612}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(lit_model, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RobustSpeechDetection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
